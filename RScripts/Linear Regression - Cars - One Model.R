# Base Source - https://www.machinelearningplus.com/machine-learning/complete-introduction-linear-regression-r/
# Complete Introduction to Linear Regression with R
# We will use the cars dataset that comes with R by default for this exercise
# cars is a simple dataset with 2 variables speed & dist that makes it convenient to show linear regression

# Problem - Predict dist when only the speed of the car is known

# Load the package that contains the full dataset and the data viz package.
library(car)
library(ggplot2)
library(MASS)
library(DAAG)

# Inspect and summarize the data.
head(cars) # First 6 rows of dataset
print(cars) # Print the entire dataset (since it's small)
# Alternative: use head(cars, n = 20) to see more rows
str(cars) # Structure of cars dataset
summary(cars) # Summarize the data of cars dataset

# Let's see the relationship between speed and dist by using scatterplot
ggplot(cars, aes(x=speed, y=dist)) + geom_point() + geom_smooth() +
  labs(title = "Distance vs Speed ScaterPlot") +
  labs(x = "Distance") +
  labs(y = "Speed")
# The scatter plot along with the smoothing line above suggests a linear and positive relationship
# between the 'dist' and 'speed'.

# Check for outliers using BoxPlot
ggplot(cars, aes(dist, speed)) + geom_boxplot()
ggplot(cars, aes(speed, dist)) + geom_boxplot()

# Another way to plot BoxPlot for identifying outliers
par(mfrow=c(1, 2))  # divide graph area in 2 columns
boxplot(cars$speed, main="Speed", sub=paste("Outlier rows: ", boxplot.stats(cars$speed)$out))  # box plot for 'speed'
boxplot(cars$dist, main="Distance", sub=paste("Outlier rows: ", boxplot.stats(cars$dist)$out))  # box plot for 'distance'

# Let's see distribution of data for speed & distance through Histogram using ggplot
ggplot(cars, aes(x=speed)) + geom_histogram() +
  labs(title = "Histogram of Speed") +
  labs(x ="Speed") + labs(y = "Frequency") +
  geom_vline(xintercept = mean(cars$speed), color="red", linetype="dashed") +
  geom_vline(xintercept = median(cars$speed), color="blue", linetype="dashed") +
  annotate("text", x = mean(cars$speed), y = Inf, label = "Mean", vjust = 2, color = "red") +
  annotate("text", x = median(cars$speed), y = Inf, label = "Median", vjust = 2, color = "blue")

ggplot(cars, aes(x=dist)) + geom_histogram() +
  labs(title = "Histogram of Distance") +
  labs(x ="Distance") + labs(y = "Frequency") +
  geom_vline(xintercept = mean(cars$dist), color="red", linetype="dashed") +
  geom_vline(xintercept = median(cars$dist), color="blue", linetype="dashed") +
  annotate("text", x = mean(cars$dist), y = Inf, label = "Mean", vjust = 2, color = "red") +
  annotate("text", x = median(cars$dist), y = Inf, label = "Median", vjust = 2, color = "blue")

# Another way to visualize distribution - Using Density Plot To Check If Response Variable Is Close To Normal
library(e1071)  # for skewness function
par(mfrow=c(1, 2))  # divide graph area in 2 columns
plot(density(cars$speed), main="Density Plot: Speed", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(cars$speed), 2)))  # density plot for 'speed'
polygon(density(cars$speed), col="red")
plot(density(cars$dist), main="Density Plot: Distance", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(cars$dist), 2)))  # density plot for 'dist'
polygon(density(cars$dist), col="red")
# Speed is very slightly negatively skewed
# Distance is highly positively skewed

# Let's see the correlation between speed and distance
# Correlation doesn't imply causation. In other words, if two variables have high correlation,
# it does not mean one variable 'causes' the value of the other variable to increase.
# Correlation is only an aid to understand the relationship.
# You can only rely on logic and business reasoning to make that judgement.
cor(cars)
# Correlation Matrix shows very high positive correlation

# Let's build the Linear Regression Model

# 1. Build Linear Regression Model on full dataset
lm_mod1 <- lm(dist ~ speed, data=cars)
print(lm_mod1)
# For the above output, you can notice the 'Coefficients' part having two components:
# Intercept: -17.579 speed: 3.932
# These are also called the beta coefficients. In other words, dist = ???17.579 + 3.932???speed

# Let's see the Diagnostics of first linear model by summarizing the model
summary(lm_mod1)
# p-Value: we can consider a linear model to be statistically significant only when
# both these p-Values are less than the pre-determined statistical significance level of 0.05
# This can visually interpreted by the significance stars at the end of the row against each X variable.
# The more the stars beside the variable's p-Value, the more significant the variable.
# Whenever there is a p-value, there is always a Null and Alternate Hypothesis associated.
# In Linear Regression, the Null Hypothesis (H0) which is p-Value > 0.5
# Null Hypothesis (H0) means that independent & dependent variable are not correlated
# The alternate hypothesis (H1) which is p-Value < 0.5
# There exists a relationship between the independent variable in question and the dependent variable.
# if the Pr(>|t|) is low, the coefficients are significant (significantly different from zero).
# If the Pr(>|t|) is high, the coefficients are not significant.

# Conclusion from the summary of first linear model
# In our case, lm_mod1, both these p-Values are well below the 0.05 threshold.
# So, we can reject the null hypothesis and conclude the model is indeed statistically significant.

# R-Squared and Adj R-Squared
# R-Squared tells us is the proportion of variation in the dependent (response) variable that has been explained by this model.
# As you add more X variables to your model, the R-Squared value of the new bigger model will always be greater than that of the smaller subset.   
# Adjusted R-Squared is formulated such that it penalises the number of terms (read predictors) in your model.
# So unlike R-sq, as the number of predictors in the model increases, the adj-R-sq may not always increase.
# Therefore when comparing nested models, it is a good practice to compare using adj-R-squared rather than just R-squared.


# Golden Rule - How to know which regression model is best fit for the data?
# The most common metrics to look at while selecting the model are:
# R-Squared	Higher the better
# Adj R-Squared	Higher the better
# F-Statistic	Higher the better
# Std. Error	Closer to zero the better
# t-statistic	Should be greater 1.96 for p-value to be less than 0.05
# AIC	Lower the better
# BIC	Lower the better
# Mallows cp	Should be close to the number of predictors in model
# MAPE (Mean absolute percentage error)	Lower the better
# MSE (Mean squared error) Lower the better
# Min_Max Accuracy => mean(min(actual, predicted)/max(actual, predicted))	Higher the better

# Prediction using training and testing dataset
# Step 1: Create Training and Test data
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # row indices for training data as 80% of full dataset
trainingData <- cars[trainingRowIndex, ]  # model training data
testData  <- cars[-trainingRowIndex, ]   # test data

# Step 2: Fit the model on training data and predict dist on test data
lm_mod2 <- lm(dist ~ speed, data=trainingData)  # build the model

# Plot Regression Line against Dataset points
ggplot() +
  geom_point(aes(x = trainingData$speed, y = trainingData$dist), color = 'red')+ 
  geom_line(aes(x = trainingData$speed, y = predict(lm_mod2, newdata = trainingData)), color = 'blue')

distPred <- predict(lm_mod2, testData)  # predict distance

# Step 3: Review diagnostic measures
summary(lm_mod2)  # model summary - Intercept = -22.657

# Step 4: Calculate prediction accuracy and error rates
actuals_preds2 <- data.frame(cbind(speed=testData$speed, actuals=testData$dist, predicteds=distPred))  # make actuals_predicteds dataframe.
correlation_accuracy2 <- cor(actuals_preds2)  
correlation_accuracy2 # 82.7%
actuals_preds2 # Accuracy is good but you can observe negative distance value has been predicted
# This is logically wrong and we will handle this in the below section

# Step 5: Calculate the Min Max accuracy and MAPE

# Min-Max Accuracy Calculation
min_max_accuracy <- mean(apply(actuals_preds2, 1, min) / apply(actuals_preds2, 1, max))
min_max_accuracy # => 38.00%, min_max accuracy

# Mean Absolute Percentage Deviation (MAPE) Calculation
mape <- mean(abs((actuals_preds2$predicteds - actuals_preds2$actuals))/actuals_preds2$actuals)
mape # => 69.95%, mean absolute percentage deviation

# Alternative 1: Using Metrics package (install if needed: install.packages("Metrics"))
# library(Metrics)
# mae(actuals_preds2$actuals, actuals_preds2$predicteds)
# rmse(actuals_preds2$actuals, actuals_preds2$predicteds)

# Alternative 2: Using caret package for comprehensive metrics
if (!require(caret, quietly = TRUE)) {
  install.packages("caret")
  library(caret)
}

# Get comprehensive regression metrics using caret
regression_metrics <- postResample(pred = actuals_preds2$predicteds, obs = actuals_preds2$actuals)
print("Regression Evaluation Metrics (using caret):")
print(regression_metrics)

# Alternative 3: Manual calculation (backup method)
mae_manual <- mean(abs(actuals_preds2$predicteds - actuals_preds2$actuals))
mse_manual <- mean((actuals_preds2$predicteds - actuals_preds2$actuals)^2)
rmse_manual <- sqrt(mse_manual)
r_squared_manual <- 1 - sum((actuals_preds2$actuals - actuals_preds2$predicteds)^2) / sum((actuals_preds2$actuals - mean(actuals_preds2$actuals))^2)

cat("\nManual Calculation Metrics:\n")
cat("MAE (Mean Absolute Error):", mae_manual, "\n")
cat("MSE (Mean Squared Error):", mse_manual, "\n") 
cat("RMSE (Root Mean Squared Error):", rmse_manual, "\n")
cat("MAPE (Mean Absolute Percentage Error):", mape, "\n")
cat("R-squared:", r_squared_manual, "\n")

# Display the actual vs predicted comparison
print("Actual vs Predicted Values:")
print(actuals_preds2)

# Step 5: k- Fold Cross validation
# One way to do this rigorous testing, is to check if the model equation performs equally well, when trained and tested on different distinct chunks of data.
# Split your data into 'k' mutually exclusive random sample portions.
# Then iteratively build k models, keeping one of k-subsets as test data each time.
# In each iteration, We build the model on the remaining (k-1 portion) data and calculate the mean squared error of the predictions on the k'th subset.
# Finally, the average of these mean squared errors (for 'k' portions) is computed.
# You need to check two things from the k-fold predictions:
# If the each of the k-fold model's prediction accuracy isn't varying too much for any one particular sample, and
# If the lines of best fit from the k-folds don't vary too much with respect the the slope and level.
# In other words, they should be parallel and as close to each other as possible.

cvResults <- suppressWarnings(CVlm(data=cars, form.lm=dist ~ speed, m=5, dots=FALSE,
                                   seed=29, legend.pos="topleft",  printit=FALSE,
                                   main="Small symbols are predicted values while bigger ones are actuals."))  # performs the CV

attr(cvResults, 'ms') # => 251.2783 mean squared error 

# Check whether dashed lines parallel and whether small & big symbols are not over dispersed for one particular color

# Step 6: Let's handle the negative predicted values of distance
new_train_data = trainingData
new_test_data = testData

# Create new variable speed.c which centers the value of the variable speed on its mean.
# This transformation was applied on speed variable so we could have a meaningful interpretation of its intercept estimate.
set.seed(1)
speed.train.c = scale(new_train_data$speed, center=TRUE, scale=FALSE)
scaled_train_speed = cbind(new_train_data$speed, speed.train.c)
scaled_train_speed
new_train_data = cbind(new_train_data, scaled_speed=speed.train.c)

speed.test.c = scale(new_test_data$speed, center=TRUE, scale=FALSE)
scaled_test_speed = cbind(new_test_data$speed, speed.test.c)
scaled_test_speed
new_test_data = cbind(new_test_data, scaled_speed=speed.test.c)

# Step 2: Fit the model on training data and predict dist on test data
lm_mod3 <- lm(dist ~ scaled_speed, data=new_train_data)  # build the model

# Plot Regression Line against Dataset points
ggplot() +
  geom_point(aes(x = new_train_data$scaled_speed, y = new_train_data$dist), color = 'red')+ 
  geom_line(aes(x = new_train_data$scaled_speed, y = predict(lm_mod3, newdata = new_train_data)), color = 'blue')

summary(lm_mod3)  # model summary - Intercept = 44.675 as speed has been scaled on mean

distPred3 <- predict(lm_mod3, new_test_data)  # predict distance

# Step 4: Calculate prediction accuracy and error rates
actuals_preds3 <- data.frame(cbind(speed=new_test_data$speed, actuals=new_test_data$dist, predicted=distPred3))  # make actuals_predicteds dataframe.
correlation_accuracy3 <- cor(actuals_preds3)  
correlation_accuracy3 # 82.7%
actuals_preds3 # Accuracy is good but you can observe negative distance value has been predicted

ggplot(new_test_data, aes(x = speed, y = dist)) +
  geom_segment(aes(xend = speed, yend = distPred3), alpha = .2) +  # Lines to connect points
  geom_point(color='blue') +  # Points of actual values
  geom_point(aes(y = distPred3), shape = 1, color='red') +  # Points of predicted values
  theme_bw()


ggplot() +
  geom_point(aes(x = trainingData$speed, y = trainingData$dist), color = 'red')+ 
  geom_line(aes(x = trainingData$speed, y = predict(lm_mod2, newdata = trainingData)), color = 'blue')

ggplot() +
  geom_point(aes(x = new_train_data$scaled_speed, y = new_train_data$dist), color = 'red')+ 
  geom_line(aes(x = new_train_data$scaled_speed, y = predict(lm_mod3, newdata = new_train_data)), color = 'blue')


